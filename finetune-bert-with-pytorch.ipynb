{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "import time\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 4,\n",
      " 'text': 'dr. goldberg offers everything i look for in a general '\n",
      "         \"practitioner.  he's nice and easy to talk to without being \"\n",
      "         \"patronizing; he's always on time in seeing his patients; he's \"\n",
      "         'affiliated with a top-notch hospital (nyu) which my parents have '\n",
      "         'explained to me is very important in case something happens and you '\n",
      "         'need surgery; and you can get referrals to see specialists without '\n",
      "         \"having to see him first.  really, what more do you need?  i'm \"\n",
      "         'sitting here trying to think of any complaints i have about him, but '\n",
      "         \"i'm really drawing a blank.\"}\n"
     ]
    }
   ],
   "source": [
    "dataset_id = 'yelp_review_full'\n",
    "dataset = load_dataset(dataset_id)\n",
    "pprint(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]),\n",
      " 'input_ids': tensor([  101,   173,  1197,   119,  2284,  2953,  3272,  1917,   178,  1440,\n",
      "         1111,  1107,   170,  1704, 22351,   119,  1119,   112,   188,  3505,\n",
      "         1105,  3123,  1106,  2037,  1106,  1443,  1217, 10063,  4404,   132,\n",
      "         1119,   112,   188,  1579,  1113,  1159,  1107,  3195,  1117,  4420,\n",
      "          132,  1119,   112,   188,  6559,  1114,   170,  1499,   118, 23555,\n",
      "         2704,   113,   183,  9379,   114,  1134,  1139,  2153,  1138,  3716,\n",
      "         1106,  1143,  1110,  1304,  1696,  1107,  1692,  1380,  5940,  1105,\n",
      "         1128,  1444,  6059,   132,  1105,  1128,  1169,  1243,  5991, 16179,\n",
      "         1106,  1267, 18137,  1443,  1515,  1106,  1267,  1140,  1148,   119,\n",
      "         1541,   117,  1184,  1167,  1202,  1128,  1444,   136,   178,   112,\n",
      "          182,  2807,  1303,  1774,  1106,  1341,  1104,  1251, 11344,   178,\n",
      "         1138,  1164,  1140,   117,  1133,   178,   112,   182,  1541,  4619,\n",
      "          170,  9153,   119,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]),\n",
      " 'labels': tensor(4),\n",
      " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['text'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column('label', 'labels')\n",
    "tokenized_dataset.set_format('torch')\n",
    "pprint(tokenized_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_subset = tokenized_dataset['train'].shuffle(seed=0).select(range(10_000))\n",
    "test_subset = tokenized_dataset['test'].shuffle(seed=0).select(range(1_000))\n",
    "\n",
    "train_dataloader = DataLoader(train_subset, shuffle=True, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_subset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af79847285b24cd784f9d155266bdb87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1/625\tloss: 1.7444\tlr: 4.992e-05\tdt: 10772.5 ms\tthroughput: 760.45 tok/s\tnorm: 0.0000\n",
      "step: 2/625\tloss: 1.7254\tlr: 4.984e-05\tdt: 8447.9 ms\tthroughput: 969.71 tok/s\tnorm: 0.0000\n",
      "step: 3/625\tloss: 1.7051\tlr: 4.976e-05\tdt: 288.9 ms\tthroughput: 28356.29 tok/s\tnorm: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5, fused=True)\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# Optimizations\n",
    "torch.cuda.empty_cache() # good practice\n",
    "torch.set_float32_matmul_precision('high') # TODO: this does nothing without bfloat16 support\n",
    "model.to(device).train()\n",
    "model = torch.compile(model)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        t0 = time.time()\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.autocast(device_type=device.type, dtype=torch.float16): # TODO: change to bfloat16 when support to prevent overflow (or use gradient scalers otherwise)\n",
    "            output = model(**batch)\n",
    "        loss = output.loss\n",
    "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        torch.cuda.synchronize() # wait for everything to finish running\n",
    "        t1 = time.time()\n",
    "        dt = (t1 - t0) * 1000\n",
    "        throughput = (batch_size * 512) / (t1 - t0)\n",
    "        print(f\"step: {progress_bar.n}/{num_training_steps}\\tloss: {loss.item():.4f}\\tlr: {optimizer.param_groups[0]['lr']:.3e}\\tdt: {dt:.1f} ms\\tthroughput: {throughput:.2f} tok/s\\tnorm: {norm:.4f}\")\n",
    "        \n",
    "        if progress_bar.n == 3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch size 4\n",
    "# step: 1/2500\tloss: 1.7454\tlr: 4.998e-05\tdt: 338.5 ms\tthroughput: 6051.09 tok/s\n",
    "# step: 2/2500\tloss: 1.8611\tlr: 4.996e-05\tdt: 294.1 ms\tthroughput: 6962.79 tok/s\n",
    "# step: 3/2500\tloss: 1.6569\tlr: 4.994e-05\tdt: 295.1 ms\tthroughput: 6940.80 tok/s\n",
    "\n",
    "# batch size 8\n",
    "# step: 1/1250\tloss: 1.5527\tlr: 4.996e-05\tdt: 813.1 ms\tthroughput: 5037.56 tok/s\n",
    "# step: 2/1250\tloss: 1.7109\tlr: 4.992e-05\tdt: 541.6 ms\tthroughput: 7562.55 tok/s\n",
    "# step: 3/1250\tloss: 1.7516\tlr: 4.988e-05\tdt: 552.3 ms\tthroughput: 7415.65 tok/s\n",
    "\n",
    "# # batch size 16\n",
    "# step: 1/625\tloss: 1.5491\tlr: 4.992e-05\tdt: 1366.7 ms\tthroughput: 5993.87 tok/s\n",
    "# step: 2/625\tloss: 1.6528\tlr: 4.984e-05\tdt: 1068.3 ms\tthroughput: 7668.55 tok/s\n",
    "# step: 3/625\tloss: 1.5627\tlr: 4.976e-05\tdt: 1064.3 ms\tthroughput: 7697.24 tok/s\n",
    "\n",
    "# batch size 32 (does not fit in vRAM anymore)\n",
    "# step: 1/313\tloss: 1.6456\tlr: 4.984e-05\tdt: 34043.3 ms\tthroughput: 481.27 tok/s\n",
    "# step: 2/313\tloss: 1.5949\tlr: 4.968e-05\tdt: 35184.1 ms\tthroughput: 465.66 tok/s\n",
    "# step: 3/313\tloss: 1.6386\tlr: 4.952e-05\tdt: 34717.0 ms\tthroughput: 471.93 tok/s\n",
    "\n",
    "# batch size 16 (with reduced matmul precision - this will be better with bfloat16 support)\n",
    "# step: 1/625\tloss: 1.5726\tlr: 4.992e-05\tdt: 1292.8 ms\tthroughput: 6336.66 tok/s\n",
    "# step: 2/625\tloss: 1.5232\tlr: 4.984e-05\tdt: 996.7 ms\tthroughput: 8219.30 tok/s\n",
    "# step: 3/625\tloss: 1.6037\tlr: 4.976e-05\tdt: 1060.8 ms\tthroughput: 7722.38 tok/s\n",
    "\n",
    "# batch size 16 (with reduced matmul precision and reduced precision)\n",
    "# step: 1/625\tloss: 1.2793\tlr: 4.992e-05\tdt: 607.1 ms\tthroughput: 13493.47 tok/s\n",
    "# step: 2/625\tloss: 1.3772\tlr: 4.984e-05\tdt: 314.9 ms\tthroughput: 26017.63 tok/s\n",
    "# step: 3/625\tloss: 1.3931\tlr: 4.976e-05\tdt: 313.7 ms\tthroughput: 26116.45 tok/s\n",
    "\n",
    "# batch size 16 (with reduced matmul precision and reduced precision and torch compile)\n",
    "# W0215 14:21:50.430000 6022 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
    "# step: 1/625\tloss: 1.3195\tlr: 4.992e-05\tdt: 29804.7 ms\tthroughput: 274.86 tok/s\n",
    "# step: 2/625\tloss: 1.2415\tlr: 4.984e-05\tdt: 295.9 ms\tthroughput: 27686.87 tok/s\n",
    "# step: 3/625\tloss: 1.0681\tlr: 4.976e-05\tdt: 281.0 ms\tthroughput: 29149.65 tok/s\n",
    "\n",
    "# batch size 16 (with reduced matmul precision and reduced precision and torch compile and fused AdamW with gradient clipping)\n",
    "# step: 1/625\tloss: 1.7444\tlr: 4.992e-05\tdt: 10772.5 ms\tthroughput: 760.45 tok/s\tnorm: 0.0000\n",
    "# step: 2/625\tloss: 1.7254\tlr: 4.984e-05\tdt: 8447.9 ms\tthroughput: 969.71 tok/s\tnorm: 0.0000\n",
    "# step: 3/625\tloss: 1.7051\tlr: 4.976e-05\tdt: 288.9 ms\tthroughput: 28356.29 tok/s\tnorm: 0.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - Adjust hyperparameters (optimizer perhaps)\n",
    "# - Gradient accumulation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

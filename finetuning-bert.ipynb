{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'text': 'My expectations for McDonalds are t rarely high. But for one to '\n",
      "         'still fail so spectacularly...that takes something special!\\\\nThe '\n",
      "         \"cashier took my friends's order, then promptly ignored me. I had to \"\n",
      "         'force myself in front of a cashier who opened his register to wait '\n",
      "         'on the person BEHIND me. I waited over five minutes for a gigantic '\n",
      "         \"order that included precisely one kid's meal. After watching two \"\n",
      "         'people who ordered after me be handed their food, I asked where mine '\n",
      "         'was. The manager started yelling at the cashiers for \\\\\"serving off '\n",
      "         'their orders\\\\\" when they didn\\'t have their food. But neither '\n",
      "         'cashier was anywhere near those controls, and the manager was the '\n",
      "         'one serving food to customers and clearing the boards.\\\\nThe manager '\n",
      "         \"was rude when giving me my order. She didn't make sure that I had \"\n",
      "         'everything ON MY RECEIPT, and never even had the decency to '\n",
      "         \"apologize that I felt I was getting poor service.\\\\nI've eaten at \"\n",
      "         \"various McDonalds restaurants for over 30 years. I've worked at more \"\n",
      "         'than one location. I expect bad days, bad moods, and the occasional '\n",
      "         'mistake. But I have yet to have a decent experience at this store. '\n",
      "         'It will remain a place I avoid unless someone in my party needs to '\n",
      "         'avoid illness from low blood sugar. Perhaps I should go back to the '\n",
      "         'racially biased service of Steak n Shake instead!'}\n"
     ]
    }
   ],
   "source": [
    "# saves the dataset to cache\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "pprint(dataset[\"train\"][100]) # example of a bad review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0\n",
      "text: My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends's order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid's meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\"serving off their orders\\\" when they didn't have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn't make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI've eaten at various McDonalds restaurants for over 30 years. I've worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!\n",
      "input_ids: [101, 1422, 11471, 1111, 9092, 1116, 1132, 189, 6034, 1344, 119, 1252, 1111, 1141, 1106, 1253, 8693, 1177, 14449, 1193, 119, 119, 119, 1115, 2274, 1380, 1957, 106, 165, 183, 1942, 4638, 5948, 2852, 1261, 1139, 2053, 112, 188, 1546, 117, 1173, 13796, 5794, 1143, 119, 146, 1125, 1106, 2049, 1991, 1107, 1524, 1104, 170, 5948, 2852, 1150, 1533, 1117, 8077, 1106, 3074, 1113, 1103, 1825, 139, 2036, 3048, 11607, 2137, 1143, 119, 146, 3932, 1166, 1421, 1904, 1111, 170, 23275, 1546, 1115, 1529, 11228, 1141, 5102, 112, 188, 7696, 119, 1258, 2903, 1160, 1234, 1150, 2802, 1170, 1143, 1129, 3541, 1147, 2094, 117, 146, 1455, 1187, 2317, 1108, 119, 1109, 2618, 1408, 13732, 1120, 1103, 5948, 11528, 1111, 165, 107, 2688, 1228, 1147, 3791, 165, 107, 1165, 1152, 1238, 112, 189, 1138, 1147, 2094, 119, 1252, 4534, 5948, 2852, 1108, 5456, 1485, 1343, 7451, 117, 1105, 1103, 2618, 1108, 1103, 1141, 2688, 2094, 1106, 5793, 1105, 8650, 1103, 8190, 119, 165, 183, 1942, 4638, 2618, 1108, 14708, 1165, 2368, 1143, 1139, 1546, 119, 1153, 1238, 112, 189, 1294, 1612, 1115, 146, 1125, 1917, 21748, 150, 3663, 155, 8231, 27514, 2101, 1942, 117, 1105, 1309, 1256, 1125, 1103, 1260, 2093, 7232, 1106, 12529, 1115, 146, 1464, 146, 1108, 2033, 2869, 1555, 119, 165, 183, 2240, 112, 1396, 8527, 1120, 1672, 9092, 1116, 7724, 1111, 1166, 1476, 1201, 119, 146, 112, 1396, 1589, 1120, 1167, 1190, 1141, 2450, 119, 146, 5363, 2213, 1552, 117, 2213, 6601, 1116, 117, 1105, 1103, 7957, 6223, 119, 1252, 146, 1138, 1870, 1106, 1138, 170, 11858, 2541, 1120, 1142, 2984, 119, 1135, 1209, 3118, 170, 1282, 146, 3644, 4895, 1800, 1107, 1139, 1710, 2993, 1106, 3644, 6946, 1121, 1822, 1892, 6656, 119, 5203, 146, 1431, 1301, 1171, 1106, 1103, 5209, 1193, 15069, 1174, 1555, 1104, 1457, 23783, 183, 25775, 1939, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "for key in tokenized_datasets['train'][100].keys():\n",
    "    print(f\"{key}: {tokenized_datasets['train'][100][key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: 0\n",
      "input_ids: tensor([  101,  1422, 11471,  1111,  9092,  1116,  1132,   189,  6034,  1344,\n",
      "          119,  1252,  1111,  1141,  1106,  1253,  8693,  1177, 14449,  1193,\n",
      "          119,   119,   119,  1115,  2274,  1380,  1957,   106,   165,   183,\n",
      "         1942,  4638,  5948,  2852,  1261,  1139,  2053,   112,   188,  1546,\n",
      "          117,  1173, 13796,  5794,  1143,   119,   146,  1125,  1106,  2049,\n",
      "         1991,  1107,  1524,  1104,   170,  5948,  2852,  1150,  1533,  1117,\n",
      "         8077,  1106,  3074,  1113,  1103,  1825,   139,  2036,  3048, 11607,\n",
      "         2137,  1143,   119,   146,  3932,  1166,  1421,  1904,  1111,   170,\n",
      "        23275,  1546,  1115,  1529, 11228,  1141,  5102,   112,   188,  7696,\n",
      "          119,  1258,  2903,  1160,  1234,  1150,  2802,  1170,  1143,  1129,\n",
      "         3541,  1147,  2094,   117,   146,  1455,  1187,  2317,  1108,   119,\n",
      "         1109,  2618,  1408, 13732,  1120,  1103,  5948, 11528,  1111,   165,\n",
      "          107,  2688,  1228,  1147,  3791,   165,   107,  1165,  1152,  1238,\n",
      "          112,   189,  1138,  1147,  2094,   119,  1252,  4534,  5948,  2852,\n",
      "         1108,  5456,  1485,  1343,  7451,   117,  1105,  1103,  2618,  1108,\n",
      "         1103,  1141,  2688,  2094,  1106,  5793,  1105,  8650,  1103,  8190,\n",
      "          119,   165,   183,  1942,  4638,  2618,  1108, 14708,  1165,  2368,\n",
      "         1143,  1139,  1546,   119,  1153,  1238,   112,   189,  1294,  1612,\n",
      "         1115,   146,  1125,  1917, 21748,   150,  3663,   155,  8231, 27514,\n",
      "         2101,  1942,   117,  1105,  1309,  1256,  1125,  1103,  1260,  2093,\n",
      "         7232,  1106, 12529,  1115,   146,  1464,   146,  1108,  2033,  2869,\n",
      "         1555,   119,   165,   183,  2240,   112,  1396,  8527,  1120,  1672,\n",
      "         9092,  1116,  7724,  1111,  1166,  1476,  1201,   119,   146,   112,\n",
      "         1396,  1589,  1120,  1167,  1190,  1141,  2450,   119,   146,  5363,\n",
      "         2213,  1552,   117,  2213,  6601,  1116,   117,  1105,  1103,  7957,\n",
      "         6223,   119,  1252,   146,  1138,  1870,  1106,  1138,   170, 11858,\n",
      "         2541,  1120,  1142,  2984,   119,  1135,  1209,  3118,   170,  1282,\n",
      "          146,  3644,  4895,  1800,  1107,  1139,  1710,  2993,  1106,  3644,\n",
      "         6946,  1121,  1822,  1892,  6656,   119,  5203,   146,  1431,  1301,\n",
      "         1171,  1106,  1103,  5209,  1193, 15069,  1174,  1555,  1104,  1457,\n",
      "        23783,   183, 25775,  1939,   106,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n",
      "token_type_ids: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "attention_mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "650000\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "for key in tokenized_datasets['train'][100].keys():\n",
    "    print(f\"{key}: {tokenized_datasets['train'][100][key]}\")\n",
    "print(len(tokenized_datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]),\n",
      " 'input_ids': tensor([[  101,   146,   112,  ...,     0,     0,     0],\n",
      "        [  101,  2098,  2276,  ...,     0,     0,     0],\n",
      "        [  101,  2950,  1104,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1422,  1873,  ...,     0,     0,     0],\n",
      "        [  101, 17623,  1165,  ...,     0,     0,     0],\n",
      "        [  101,  1422,  1560,  ...,     0,     0,     0]]),\n",
      " 'labels': tensor([2, 0, 0, 3, 1, 2, 0, 4, 1, 0, 1, 2, 4, 0, 2, 1]),\n",
      " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n",
      "torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(10_000))\n",
    "small_test_dataset = tokenized_datasets['test'].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=16)\n",
    "eval_dataloader = DataLoader(small_test_dataset, batch_size=16)\n",
    "\n",
    "pprint(next(iter(train_dataloader)))\n",
    "pprint(next(iter(train_dataloader))['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=5)\n",
    "\n",
    "pprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28996, 768]) True\n",
      "torch.Size([512, 768]) True\n",
      "torch.Size([2, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([3072]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([3072]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([3072]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([3072]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([3072]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([3072]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([3072]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([3072]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([3072]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([3072]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([3072]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([3072, 768]) True\n",
      "torch.Size([3072]) True\n",
      "torch.Size([768, 3072]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([768, 768]) True\n",
      "torch.Size([768]) True\n",
      "torch.Size([5, 768]) True\n",
      "torch.Size([5]) True\n"
     ]
    }
   ],
   "source": [
    "# we are training all params\n",
    "for p in model.parameters():\n",
    "    print(p.shape, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.lr_scheduler.LambdaLR"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "type(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0f0d4431384c818d65e167d8c0e1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()} # send tensors to device\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.631}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
